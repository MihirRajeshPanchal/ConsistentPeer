{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-23T11:21:28.644167Z",
     "iopub.status.busy": "2025-02-23T11:21:28.643913Z",
     "iopub.status.idle": "2025-02-23T11:22:26.354541Z",
     "shell.execute_reply": "2025-02-23T11:22:26.353698Z",
     "shell.execute_reply.started": "2025-02-23T11:21:28.644139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.12)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.11.0a2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.22.4->langchain) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.22.4->langchain) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.22.4->langchain) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Installing collected packages: async-timeout\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 5.0.1\n",
      "    Uninstalling async-timeout-5.0.1:\n",
      "      Successfully uninstalled async-timeout-5.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed async-timeout-4.0.3\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.35 (from langchain_openai)\n",
      "  Downloading langchain_core-0.3.37-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting openai<2.0.0,>=1.58.1 (from langchain_openai)\n",
      "  Downloading openai-1.64.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.9.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (0.2.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain_openai) (2.11.0a2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.35->langchain_openai) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.35->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.35->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.35->langchain_openai) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.3.0)\n",
      "Downloading langchain_openai-0.3.6-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.37-py3-none-any.whl (413 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.64.0-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai, langchain-core, langchain_openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.4\n",
      "    Uninstalling openai-1.57.4:\n",
      "      Successfully uninstalled openai-1.57.4\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.25\n",
      "    Uninstalling langchain-core-0.3.25:\n",
      "      Successfully uninstalled langchain-core-0.3.25\n",
      "Successfully installed langchain-core-0.3.37 langchain_openai-0.3.6 openai-1.64.0\n",
      "Collecting langchain_neo4j\n",
      "  Downloading langchain_neo4j-0.3.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from langchain_neo4j) (0.3.12)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from langchain_neo4j) (0.3.37)\n",
      "Collecting neo4j<6.0.0,>=5.25.0 (from langchain_neo4j)\n",
      "  Downloading neo4j-5.28.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (3.11.12)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (0.2.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (2.11.0a2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.7->langchain_neo4j) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.8->langchain_neo4j) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.8->langchain_neo4j) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.8->langchain_neo4j) (4.12.2)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j<6.0.0,>=5.25.0->langchain_neo4j) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.4.0,>=0.3.7->langchain_neo4j) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.4.0,>=0.3.7->langchain_neo4j) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.4.0,>=0.3.7->langchain_neo4j) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.18.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain_neo4j) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain_neo4j) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain_neo4j) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.0.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.7->langchain_neo4j) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.7->langchain_neo4j) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain_neo4j) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain_neo4j) (0.14.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.22.4->langchain<0.4.0,>=0.3.7->langchain_neo4j) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain<0.4.0,>=0.3.7->langchain_neo4j) (1.2.2)\n",
      "Downloading langchain_neo4j-0.3.0-py3-none-any.whl (38 kB)\n",
      "Downloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: neo4j, langchain_neo4j\n",
      "Successfully installed langchain_neo4j-0.3.0 neo4j-5.28.1\n",
      "Collecting fastapi[all]\n",
      "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi[all])\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (2.11.0a2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (4.12.2)\n",
      "Collecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
      "  Downloading fastapi_cli-0.0.7-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (0.28.1)\n",
      "Collecting jinja2>=3.1.5 (from fastapi[all])\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-multipart>=0.0.18 (from fastapi[all])\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (2.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (6.0.2)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (5.10.0)\n",
      "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (3.10.12)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (2.2.0)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pydantic-settings>=2.0.0 (from fastapi[all])\n",
      "  Downloading pydantic_settings-2.8.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pydantic-extra-types>=2.0.0 (from fastapi[all])\n",
      "  Downloading pydantic_extra_types-2.10.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=2.0.0->fastapi[all]) (2.7.0)\n",
      "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email-validator>=2.0.0->fastapi[all]) (3.10)\n",
      "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (0.15.1)\n",
      "Collecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all])\n",
      "  Downloading rich_toolkit-0.13.2-py3-none-any.whl.metadata (999 bytes)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi[all]) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi[all]) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi[all]) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[all]) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.5->fastapi[all]) (3.0.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[all]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi[all]) (2.29.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.0->fastapi[all])\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.12.0->uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]) (8.1.7)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all])\n",
      "  Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]) (14.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi[all]) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi[all]) (1.2.2)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.10/dist-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (13.9.4)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"all\"->fastapi[all]) (0.1.2)\n",
      "Downloading fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_extra_types-2.10.2-py3-none-any.whl (35 kB)\n",
      "Downloading pydantic_settings-2.8.0-py3-none-any.whl (30 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading rich_toolkit-0.13.2-py3-none-any.whl (13 kB)\n",
      "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uvloop, uvicorn, python-multipart, python-dotenv, jinja2, httptools, watchfiles, starlette, rich-toolkit, pydantic-settings, pydantic-extra-types, fastapi, fastapi-cli\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "Successfully installed fastapi-0.115.8 fastapi-cli-0.0.7 httptools-0.6.4 jinja2-3.1.5 pydantic-extra-types-2.10.2 pydantic-settings-2.8.0 python-dotenv-1.0.1 python-multipart-0.0.20 rich-toolkit-0.13.2 starlette-0.45.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.37)\n",
      "Collecting langchain<1.0.0,>=0.3.19 (from langchain_community)\n",
      "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.12)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.8.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain<1.0.0,>=0.3.19->langchain_community)\n",
      "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (2.11.0a2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain_community) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain_community) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain_community) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain_community) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain_community) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.26.4->langchain_community) (2.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (2.29.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.26.4->langchain_community) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.26.4->langchain_community) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.26.4->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.26.4->langchain_community) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.26.4->langchain_community) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.2.2)\n",
      "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain-0.3.19-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: httpx-sse, langchain-text-splitters, langchain, langchain_community\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.3\n",
      "    Uninstalling langchain-text-splitters-0.3.3:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.3\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.12\n",
      "    Uninstalling langchain-0.3.12:\n",
      "      Successfully uninstalled langchain-0.3.12\n",
      "Successfully installed httpx-sse-0.4.0 langchain-0.3.19 langchain-text-splitters-0.3.6 langchain_community-0.3.18\n",
      "Collecting jq\n",
      "  Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Downloading jq-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jq\n",
      "Successfully installed jq-1.8.0\n",
      "Collecting certainty-estimator\n",
      "  Downloading certainty_estimator-1.6-py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading certainty_estimator-1.6-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: certainty-estimator\n",
      "Successfully installed certainty-estimator-1.6\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Collecting simpletransformers\n",
      "  Downloading simpletransformers-0.70.1-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.67.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2024.11.6)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.47.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (3.3.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
      "Collecting seqeval (from simpletransformers)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.17.1)\n",
      "Collecting tensorboardx (from simpletransformers)\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.2.3)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.21.0)\n",
      "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.19.1)\n",
      "Collecting streamlit (from simpletransformers)\n",
      "  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.29.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->simpletransformers) (0.4.5)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->simpletransformers) (2.4.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (2.11.0a2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2025.1.31)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->simpletransformers) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.11.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.5.0)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (11.0.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.0.0)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit->simpletransformers)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.68.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.7)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.1.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (1.18.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb>=0.10.32->simpletransformers) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb>=0.10.32->simpletransformers) (2.29.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->simpletransformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->simpletransformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->simpletransformers) (2024.2.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->simpletransformers) (2024.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.22.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n",
      "Downloading simpletransformers-0.70.1-py3-none-any.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=1795086533c4a81a27fafca540ca4be9ee094436132ee5bbcd7f7d05d29d726e\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: pydeck, tensorboardx, streamlit, seqeval, simpletransformers\n",
      "Successfully installed pydeck-0.9.1 seqeval-1.2.2 simpletransformers-0.70.1 streamlit-1.42.2 tensorboardx-2.6.2.2\n",
      "Collecting neo4j_graphrag\n",
      "  Downloading neo4j_graphrag-1.4.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: fsspec<2025.0.0,>=2024.9.0 in /usr/local/lib/python3.10/dist-packages (from neo4j_graphrag) (2024.12.0)\n",
      "Collecting json-repair<0.31.0,>=0.30.2 (from neo4j_graphrag)\n",
      "  Downloading json_repair-0.30.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: neo4j<6.0.0,>=5.17.0 in /usr/local/lib/python3.10/dist-packages (from neo4j_graphrag) (5.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from neo4j_graphrag) (2.11.0a2)\n",
      "Collecting pypdf<5.0.0,>=4.3.1 (from neo4j_graphrag)\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from neo4j_graphrag) (6.0.2)\n",
      "Collecting types-pyyaml<7.0.0.0,>=6.0.12.20240917 (from neo4j_graphrag)\n",
      "  Downloading types_PyYAML-6.0.12.20241230-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j<6.0.0,>=5.17.0->neo4j_graphrag) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->neo4j_graphrag) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->neo4j_graphrag) (2.29.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->neo4j_graphrag) (4.12.2)\n",
      "Downloading neo4j_graphrag-1.4.3-py3-none-any.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.1/152.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading json_repair-0.30.3-py3-none-any.whl (18 kB)\n",
      "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_PyYAML-6.0.12.20241230-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: types-pyyaml, pypdf, json-repair, neo4j_graphrag\n",
      "  Attempting uninstall: pypdf\n",
      "    Found existing installation: pypdf 5.3.0\n",
      "    Uninstalling pypdf-5.3.0:\n",
      "      Successfully uninstalled pypdf-5.3.0\n",
      "Successfully installed json-repair-0.30.3 neo4j_graphrag-1.4.3 pypdf-4.3.1 types-pyyaml-6.0.12.20241230\n"
     ]
    }
   ],
   "source": [
    "!pip install  langchain\n",
    "!pip install  langchain_openai\n",
    "!pip install  langchain_neo4j\n",
    "!pip install  fastapi[all]\n",
    "!pip install  requests\n",
    "!pip install  python-dotenv\n",
    "!pip install  langchain_community\n",
    "!pip install  jq\n",
    "!pip install  certainty-estimator\n",
    "!pip install  transformers\n",
    "!pip install  simpletransformers\n",
    "!pip install neo4j_graphrag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:22:26.355781Z",
     "iopub.status.busy": "2025-02-23T11:22:26.355557Z",
     "iopub.status.idle": "2025-02-23T11:23:14.132901Z",
     "shell.execute_reply": "2025-02-23T11:23:14.132235Z",
     "shell.execute_reply.started": "2025-02-23T11:22:26.355761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from certainty_estimator.predict_certainty import CertaintyEstimator\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from simpletransformers.ner import NERModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from neo4j_graphrag.retrievers import HybridRetriever\n",
    "from neo4j_graphrag.llm import OpenAILLM\n",
    "from neo4j_graphrag.generation import GraphRAG\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "from certainty_estimator.predict_certainty import CertaintyEstimator\n",
    "from simpletransformers.ner import NERModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:14.134803Z",
     "iopub.status.busy": "2025-02-23T11:23:14.134160Z",
     "iopub.status.idle": "2025-02-23T11:23:33.378185Z",
     "shell.execute_reply": "2025-02-23T11:23:33.376881Z",
     "shell.execute_reply.started": "2025-02-23T11:23:14.134775Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:810: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8072546acf7a4e29bc555800b864ab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/423 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f52d16e4464298ba46bb14805c4204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/712 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b0d4a71e5f48cd8341c5d61b46b3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3f4485a561476486c3e9e821325abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3927c2629c804673b3e595ff167b7b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd063d59c0c4b9e8f09f5190b81abf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4690235e8c8498f9b63f28ea11a41d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0687c1d92ee4c34a8d4cf16a907d2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11a1a7b21574862b81f17819449f652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55dcc4946fc14e81b879c400089f00fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c277edb34745aea6579d0e6c7878dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/338 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f1dcadf3704d8c92b63a7c93f96e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60635f9502b418da57e74975002d64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "bpe.codes:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8dbb7fbe4f419797e3f3a93439befd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/22.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43178c8f0171484aa18b6a18acf6a1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5179977b2f0b46c69e7dcc96fb5a574d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/949 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ad352671274e74b773df9717b43244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimator = CertaintyEstimator(\"sentence-level\", cuda=True)\n",
    "hedge_model = NERModel(\n",
    "    \"bert\",\n",
    "    \"jeniakim/hedgehog\",\n",
    "    use_cuda=False,\n",
    "    labels=[\"C\", \"D\", \"E\", \"I\", \"N\"],\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")\n",
    "conviction_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:33.383054Z",
     "iopub.status.busy": "2025-02-23T11:23:33.382758Z",
     "iopub.status.idle": "2025-02-23T11:23:33.390520Z",
     "shell.execute_reply": "2025-02-23T11:23:33.389564Z",
     "shell.execute_reply.started": "2025-02-23T11:23:33.383014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_hedge(text):\n",
    "\n",
    "    predictions, _ = hedge_model.predict([text])\n",
    "\n",
    "    token_predictions = predictions[0]\n",
    "\n",
    "    uncertainty_labels = {\"D\", \"E\", \"I\", \"N\"}\n",
    "\n",
    "    uncertainty_count = sum(\n",
    "        1\n",
    "        for token_dict in token_predictions\n",
    "        if list(token_dict.values())[0] in uncertainty_labels\n",
    "    )\n",
    "\n",
    "    total_tokens = len(token_predictions)\n",
    "    hedge_score = uncertainty_count / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    return hedge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:33.392400Z",
     "iopub.status.busy": "2025-02-23T11:23:33.392067Z",
     "iopub.status.idle": "2025-02-23T11:23:35.003018Z",
     "shell.execute_reply": "2025-02-23T11:23:35.002070Z",
     "shell.execute_reply.started": "2025-02-23T11:23:33.392369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_conviction_score(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = conviction_model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "\n",
    "    probs = F.softmax(logits, dim=-1).squeeze()\n",
    "\n",
    "    conviction_scores = torch.tensor([-1.0, 0.0, 1.0])\n",
    "\n",
    "    conviction_score = torch.dot(probs, conviction_scores).item()\n",
    "\n",
    "    return conviction_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:35.004359Z",
     "iopub.status.busy": "2025-02-23T11:23:35.004062Z",
     "iopub.status.idle": "2025-02-23T11:23:35.015785Z",
     "shell.execute_reply": "2025-02-23T11:23:35.014620Z",
     "shell.execute_reply.started": "2025-02-23T11:23:35.004330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_certainity(text):\n",
    "    return estimator.predict(text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:35.019751Z",
     "iopub.status.busy": "2025-02-23T11:23:35.019421Z",
     "iopub.status.idle": "2025-02-23T11:23:35.028410Z",
     "shell.execute_reply": "2025-02-23T11:23:35.027554Z",
     "shell.execute_reply.started": "2025-02-23T11:23:35.019707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_everything(text):\n",
    "    hedge = get_hedge(text)\n",
    "    conviction_score = get_conviction_score(text)\n",
    "    certainity = get_certainity(text)\n",
    "\n",
    "    return hedge, conviction_score, certainity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:35.029927Z",
     "iopub.status.busy": "2025-02-23T11:23:35.029725Z",
     "iopub.status.idle": "2025-02-23T11:23:35.041584Z",
     "shell.execute_reply": "2025-02-23T11:23:35.040630Z",
     "shell.execute_reply.started": "2025-02-23T11:23:35.029910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "review = \"This paper considers a special deep learning model and shows that in expectation, there is only one unique local minimizer. As a result, a gradient descent algorithm converges to the unique solution. This works address a conjecture proposed by Tian (2017). While it is clearly written, my main concern is whether this model is significant enough. The assumptions K=2 and v1=v2=1 reduces the difficulty of the analysis, but it makes the model considerably simpler than any practical setting.\"\n",
    "rating = \"6: Marginally above acceptance threshold\"\n",
    "rating_score = 6\n",
    "confidence = \"2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper\"\n",
    "confidence_score = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:35.042875Z",
     "iopub.status.busy": "2025-02-23T11:23:35.042528Z",
     "iopub.status.idle": "2025-02-23T11:23:36.103043Z",
     "shell.execute_reply": "2025-02-23T11:23:36.102091Z",
     "shell.execute_reply.started": "2025-02-23T11:23:35.042844Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d7175f760d4cfc8fb3508d91f600ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e825c88f8a3f4cdf9e4fc7817e67e5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01282051282051282, 0.41252410411834717, 4.7804375)\n"
     ]
    }
   ],
   "source": [
    "print(get_everything(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:36.104363Z",
     "iopub.status.busy": "2025-02-23T11:23:36.104025Z",
     "iopub.status.idle": "2025-02-23T11:23:36.108368Z",
     "shell.execute_reply": "2025-02-23T11:23:36.107460Z",
     "shell.execute_reply.started": "2025-02-23T11:23:36.104328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NEO4J_URI = \"\"\n",
    "NEO4J_USERNAME = \"\"\n",
    "NEO4J_PASSWORD = \"\"\n",
    "OPENAI_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:36.109517Z",
     "iopub.status.busy": "2025-02-23T11:23:36.109256Z",
     "iopub.status.idle": "2025-02-23T11:23:36.121999Z",
     "shell.execute_reply": "2025-02-23T11:23:36.121334Z",
     "shell.execute_reply.started": "2025-02-23T11:23:36.109489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:36.123077Z",
     "iopub.status.busy": "2025-02-23T11:23:36.122794Z",
     "iopub.status.idle": "2025-02-23T11:23:36.251158Z",
     "shell.execute_reply": "2025-02-23T11:23:36.249907Z",
     "shell.execute_reply.started": "2025-02-23T11:23:36.123048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:36.252375Z",
     "iopub.status.busy": "2025-02-23T11:23:36.252083Z",
     "iopub.status.idle": "2025-02-23T11:23:39.459707Z",
     "shell.execute_reply": "2025-02-23T11:23:39.458789Z",
     "shell.execute_reply.started": "2025-02-23T11:23:36.252344Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "retriever = HybridRetriever(\n",
    "    driver=driver,\n",
    "    vector_index_name=\"reviewEmbeddings\",\n",
    "    fulltext_index_name=\"reviewFullText\",\n",
    "    embedder=embeddings,\n",
    "    return_properties=[\n",
    "        \"review\",\n",
    "        \"certainty\",\n",
    "        \"conviction\",\n",
    "        \"hedge\",\n",
    "        \"id\",\n",
    "        \"confidence\",\n",
    "        \"rating\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "llm = OpenAILLM(model_name=\"gpt-4o\", model_params={\"temperature\": 0})\n",
    "rag = GraphRAG(retriever=retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:39.460932Z",
     "iopub.status.busy": "2025-02-23T11:23:39.460677Z",
     "iopub.status.idle": "2025-02-23T11:23:39.467743Z",
     "shell.execute_reply": "2025-02-23T11:23:39.466962Z",
     "shell.execute_reply.started": "2025-02-23T11:23:39.460911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Review(BaseModel):\n",
    "    review: str\n",
    "    rating: str\n",
    "    rating_score: int\n",
    "    confidence: str\n",
    "    confidence_score: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:39.468991Z",
     "iopub.status.busy": "2025-02-23T11:23:39.468716Z",
     "iopub.status.idle": "2025-02-23T11:23:39.685708Z",
     "shell.execute_reply": "2025-02-23T11:23:39.684910Z",
     "shell.execute_reply.started": "2025-02-23T11:23:39.468970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rev1 = Review(\n",
    "    review=review,\n",
    "    rating=rating,\n",
    "    rating_score=rating_score,\n",
    "    confidence=confidence,\n",
    "    confidence_score=confidence_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:39.686791Z",
     "iopub.status.busy": "2025-02-23T11:23:39.686512Z",
     "iopub.status.idle": "2025-02-23T11:23:39.697815Z",
     "shell.execute_reply": "2025-02-23T11:23:39.697041Z",
     "shell.execute_reply.started": "2025-02-23T11:23:39.686767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_consistency(conviction, certainty, rating, confidence, hedge):\n",
    "    \"\"\"\n",
    "    Compute the Consistency score based on given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    conviction (float): Conviction (-1 to 1)\n",
    "    certainty (float): Certainty (3.3165 to 4.9329)\n",
    "    rating (float): Rating (6 to 9)\n",
    "    confidence (float): Confidence (2 to 5)\n",
    "    hedge (float): Hedge (0 to 0.0734)\n",
    "\n",
    "    Returns:\n",
    "    float: Consistency score (1 to 10)\n",
    "    \"\"\"\n",
    "    conviction = max(-1, min(1, conviction))\n",
    "    certainty = max(3.3165, min(4.9329, certainty))\n",
    "    rating = max(6, min(9, rating))\n",
    "    confidence = max(2, min(5, confidence))\n",
    "    hedge = max(0, min(0.0734, hedge))\n",
    "\n",
    "    consistency = 1 + 9 * (\n",
    "        0.4 * ((certainty - 3.3165) / 1.6164)\n",
    "        + (0.25 + 0.1 * ((rating - 6) / 3) + 0.1 * ((confidence - 2) / 3))\n",
    "        * ((conviction + 1) / 2)\n",
    "        + 0.15 * ((rating - 6) / 3)\n",
    "        + 0.1 * ((confidence - 2) / 3)\n",
    "        - 0.1 * (hedge / 0.0734)\n",
    "    )\n",
    "\n",
    "    return max(1, min(10, consistency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:23:39.699161Z",
     "iopub.status.busy": "2025-02-23T11:23:39.698851Z",
     "iopub.status.idle": "2025-02-23T11:23:40.200286Z",
     "shell.execute_reply": "2025-02-23T11:23:40.199056Z",
     "shell.execute_reply.started": "2025-02-23T11:23:39.699131Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d520a84fc62d4f86acf89bb260f346fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db72fb7adffa4bd2b5a9e7caf8b2c087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.692329667054569"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_consistency(\n",
    "    get_conviction_score(rev1.review),\n",
    "    get_certainity(rev1.review),\n",
    "    rev1.rating_score,\n",
    "    rev1.confidence_score,\n",
    "    get_hedge(rev1.review),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:31:54.392276Z",
     "iopub.status.busy": "2025-02-23T11:31:54.391911Z",
     "iopub.status.idle": "2025-02-23T11:31:54.396911Z",
     "shell.execute_reply": "2025-02-23T11:31:54.395968Z",
     "shell.execute_reply.started": "2025-02-23T11:31:54.392246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def nlp(request: Review):\n",
    "    try:\n",
    "        request = (\n",
    "            \"Analyze the similar reviews and their confidence and rating score and give counterfactual reasoning for given review \"\n",
    "            + request.review\n",
    "            + \"with confidence score \"\n",
    "            + request.confidence\n",
    "            + \"and rating score \"\n",
    "            + request.rating\n",
    "        )\n",
    "        retriever_result = rag.search(\n",
    "            query_text=request, return_context=True, retriever_config={\"top_k\": 5}\n",
    "        )\n",
    "        return {\"result\": retriever_result}\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:31:54.720307Z",
     "iopub.status.busy": "2025-02-23T11:31:54.720068Z",
     "iopub.status.idle": "2025-02-23T11:32:05.254283Z",
     "shell.execute_reply": "2025-02-23T11:32:05.253624Z",
     "shell.execute_reply.started": "2025-02-23T11:31:54.720286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "res = nlp(rev1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:32:05.255764Z",
     "iopub.status.busy": "2025-02-23T11:32:05.255452Z",
     "iopub.status.idle": "2025-02-23T11:32:05.260309Z",
     "shell.execute_reply": "2025-02-23T11:32:05.259634Z",
     "shell.execute_reply.started": "2025-02-23T11:32:05.255732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': RagResultModel(answer=\"The given review expresses concerns about the significance and practicality of the model being studied, despite acknowledging that the paper is clearly written and addresses a conjecture proposed by Tian (2017). The review has a confidence score of 2, indicating that the reviewer is willing to defend their evaluation but may not have fully understood central parts of the paper. The rating score of 6 suggests that the paper is marginally above the acceptance threshold.\\n\\nTo provide counterfactual reasoning, we can analyze similar reviews with varying confidence and rating scores:\\n\\n1. **ICLR_2018_133**: This review has a confidence score of 4 and a rating of 6. The reviewer acknowledges the novelty of the proposed method and its potential for improved performance in certain tasks, but also highlights areas where the method does not perform as expected. The review suggests that addressing these issues could improve the paper's acceptance prospects.\\n\\n2. **ICLR_2017_129**: With a confidence score of 4 and a rating of 6, this review appreciates the technical soundness of the paper but notes limited novelty. The reviewer suggests that the paper would be more valuable if it included released code or novel insights that improve training scalability.\\n\\n3. **ICLR_2019_1001**: This review has a confidence score of 4 and a rating of 5. The reviewer finds the proposed approach simple but raises concerns about the claims made and the lack of quantitative experimental results. The review suggests that addressing these issues and providing more robust experiments could strengthen the paper.\\n\\nCounterfactual reasoning for the given review could involve considering how the paper might be perceived if it addressed the concerns about the model's significance and practicality. For instance, if the paper demonstrated the model's applicability to more complex or practical settings, or if it provided additional insights or experiments that highlight the model's relevance, the reviewer's confidence and rating might increase. Additionally, if the reviewer had a better understanding of the paper's central parts, their confidence score might improve, leading to a more favorable evaluation.\", retriever_result=RetrieverResult(items=[RetrieverResultItem(content=\"{'id': 'ICLR_2017_12', 'hedge': 0.04081632653061224, 'certainty': 4.488500595092773, 'conviction': 0.45622146129608154, 'confidence': 3, 'rating': 10, 'review': 'The paper is an empirical study to justify that 1 SGD with smaller batch sizes converges to flatter minima 2 flatter minima have better generalization ability Pros and ConsAlthough there is little novelty in the paper I think the work is of great value in shedding light into some interesting questions around generalization of deep networks SignificanceI think such results may have impact on both theory and practice respectively by suggesting what assumptions are legitimate for real scenarios for building new theories or be used heuristically to develop new algorithms with generalization by smart manipulation of minibatch sizesCommentsEarlier I had some concern about the correctness of a claim made by the authors which is resolved now They had claimed their proposed sharpness criterion is scale invariance They took care of it by removing this claim in the revised version'}\", metadata={'score': 1.0}), RetrieverResultItem(content=\"{'id': 'ICLR_2018_133', 'hedge': 0.0, 'certainty': 4.51829719543457, 'conviction': 0.09816280007362366, 'confidence': 4, 'rating': 6, 'review': '1 SummaryThis paper proposes a recurrent neural network RNN training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably The authors propose to train a forward and backward RNN in parallel The forward RNN predicts forward in time and the backward RNN predicts backwards in time While the forward RNN is trained to predict the next timestep its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step In experiments it is shown that the proposed method improves training speed in terms of number of training iterations achieves 08 CIDEr points improvement over baselines using the proposed training and also achieves improved performance for the task of speech recognition2 Pros Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected Improved performance in speech recognition task The idea is clearly explained and well motivated3 ConsImage captioning experimentIn the experimental section there is an image captioning result in which the proposed method is used on top of two baselines This experiment shows improvement over such baselines however the performance is still worse compared against baselines such as Lu et al 2017 and Yao et al 2016 It would be optimal if the authors can use their training method on such baselines and show improved performance or explain why this cannot be doneUnconditioned generation experimentsIn these experiments sequential pixelbypixel MNIST generation is performed in which the proposed method did not help Because of this two conditioned set ups are performed 1 25 of pixels are given before generation and 2 75 of pixels are given before generation The proposed method performs similar to the baseline in the 25 case and better than the baseline in the 75 case For completeness and to come to a stronger conclusion on how much uncertainty really affects the proposed method this experiment needs a case in which 50 of the pixels are given Observing 25 of the pixels gives almost no information about the identity of the digit and it makes sense that its hard to encode the future however 50 of the pixels give a good idea of what the digit identity is If the authors believe that the 50 case is not necessary please feel free to explain whyAdditional commentsThe method is shown to converge faster compared to the baselines however it is possible that the baseline may finish training faster the authors do acknowledge the additional computation needed in the backward RNNIt would be informative for the research community to see the relationship of training time how long it takes in hours versus how fast it learns iterations taken to learnExperiments on RL planning tasks would be interesting to see Maybe on a simplepredictable environment4 ConclusionThe paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past The proposed method presents a possible way of better modeling the future however some the results do not clearly back up the claim yet The given score will improve if the authors are able to address the stated issuesPOST REBUTTAL RESPONSEThe authors have addressed the comments on the MNIST experiments and show better results however as far as I can see they did not address my concern about the comparisons on the image captioning experiment In the image captioning experiment the authors choose two networks Show  Tell and Soft attention that they improve using the proposed method that end up performing similar to the second best baseline Yao et al 2016 based on Table 3 and their response I requested for the authors to use their method on the best performing baselines ie Yao et al 2016 or Liu et al 2017 or explain why this cannot be done maybe my request was not clearly stated Applying the proposed method on the strong baselines would highlight the authors claims more strongly than just applying on the average performing chosen baselines This request was not addressed and instead the authors just improved the average performing baselines in Table 3 to meet the best baselines Given that the authors were able to improve the results in the sequential MNIST and improve the average baselines my rating improves one point However I still have concerns about this method not being shown to improve the best methods presented in Table 3 which would give a more solid result My rating changes to marginally above threshold for acceptance'}\", metadata={'score': 1.0}), RetrieverResultItem(content=\"{'id': 'ICLR_2017_129', 'hedge': 0.042735042735042736, 'certainty': 4.968155860900879, 'conviction': 0.11820875853300095, 'confidence': 4, 'rating': 6, 'review': 'This paper is technically sound It highlights well the strengths and weaknesses of the proposed simplified modelIn terms of impact its novelty is limited in the sense that the authors did seemingly the right thing and obtained the expected outcomes The idea of modeling deep learning computation is not in itself particularly novel As a companion paper to an open source release of the model it would meet my bar of acceptance in the same vein as a paper describing a novel dataset which might not provide groundbreaking insights yet be generally useful to the communityIn the absence of released code even if the authors promise to release it soon I am more ambivalent since thats where all the value lies It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way and incorporated such new insights in the paperUPDATED code is now available Revised review accordingly'}\", metadata={'score': 0.996453260209192}), RetrieverResultItem(content=\"{'id': 'ICLR_2019_1032', 'hedge': 0.054945054945054944, 'certainty': 3.0230276584625244, 'conviction': -0.28229820728302, 'confidence': 4, 'rating': 5, 'review': 'This paper studies ReLU model or equivalently onelayeroneneuron model for the classification problem This paper shows if the data is linearly separable gradient descent may converge to either a global minimum or a suboptimal local minimum or diverges This paper further studies the implicit bias induced by GD and SGD and shows if they converge they can have a maximum margin solution Comments1 Using ReLU model for linearly separable data doesnt make sense to me When ReLU is used I expect some more complicated separable condition 2 This paper only studies onelayeroneneuron model which is a very restricted setting Its hard to see how this result can be generalized to the multipleneuron case3 The analysis follows closely with previous work in studying the implicit bias for linear models'}\", metadata={'score': 0.9959934976437168}), RetrieverResultItem(content=\"{'id': 'ICLR_2019_1001', 'hedge': 0.008620689655172414, 'certainty': 4.723076343536377, 'conviction': 0.013329338282346725, 'confidence': 4, 'rating': 5, 'review': 'The paper proposes an approach to provide contrastive visual explanations for deep neural networks  why the network assigned more confidence to some class A as opposed to some other class B As opposed to the applicability of previous approaches to this problem  the approach is designed to directly answer the contrastive explanations question rather adapting other visual saliency techniques for the same Overall while I find the proposed approach simple  the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same Apart from some flaws in the claims made in the paper the paper is easy to follow and understand Assuming the availability of a latent model over the images of the input distribution the proposed approach is directly applicable and faster The authors clearly highlight the problems associated with existing explanation modalities and approaches ranging from ones applicable to only specific deep architectures to ones using backpropagation based heuristics The proposed approach to generate contrastive explanations is simple and is structured along the lines of methods utilizing probe images to explain decisions  except for the added advantage that the provided explanations are instanceagnostic due to the assumption of a latent model over the input distributionComments One of the problems highlighted in the paper regarding existing explanation modalities is the use of another blackbox to explain the decisions of an existing deep network also somewhat of a blackbox which the authors claim their model does not suffer from The proposed approach provides explanations by operating in the latent space of a learned generative model of the input distribution The learned generator in itself is somewhat of a blackbox itself  there has been prior work indicating how much of the input distribution are GANs able to capture As such conditioning on a generative model to propose such contrastive explanations is to some extent using another blackbox generator to explain the decisions of an existing one Thus the above claim made in the paper does not seem wellfounded Furthermore in experiments the paper does not provide any quantitatively convincing results to suggest the generator in use is a good one While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable offtheshelf for any further contrastive explanations regarding any network operating on the same dataset  learning such a model of the input space is an overhead in itself In this light experiments demonstrating comparisons between GANs and VAEs as the reference generative model for explanations would have made the paper stronger as the proposed approach relies explicitly on how good the generative model is  The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain adding square to the topleft of images of class 8 While I like this experiment I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations Typographical Errors Section 31 repeats the use of D for a discriminator as well as the input distribution Procedure 1 and Procedure 2 share the same titles  which is slightly misleading In addition Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same In Section 41 the use of Gradcam and Lime to generate counterfactual explanations is not very clear and makes it slightly hard to follow Citations used for Gradcam are wrong  Sundarajan et al 2016 should be changed to Selvaraju et al 2017Experimental Issues Experimental results are provided only on MNIST and FashionMNIST Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B  experiments on datasets which do not have realimages seem insufficient Additional experiments on at least ImageNet would have made the paper strongerRegarding contrastive explanations experiments on datasets where distractor classes yprobe are present in addition to the class interest ytrue seem important  PASCAL VOC COCO etc Specifically since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting  what needs to change in a region of an image classified as a cat to be classified as a dog while there is an instance of the class  dog present in the image itself Also section 7 in Gradcam httpsarxivorgpdf161002391pdf provides a procedure to generate counterfactual explanations using Gradcam Is there a particular reason the authors did not choose to adopt the above technique as a baseline Experimental results provided in the paper are only qualitative  as such I do not find the comparisons and improvements over the existing approaches convincing enough Since there is no clear metric to evaluate contrastive explanations  human studies to judge the classdiscriminativeness or trust of the proposed approach would have made the paper strongerThe authors adressed the issues raisedcomments made in the review In light of my comments below to the author responses  I am not inclined towards increasing my rating and will stick to my original rating for the paper'}\", metadata={'score': 0.9936782647247172})], metadata={'__retriever': 'HybridRetriever'}))}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:45:21.299735Z",
     "iopub.status.busy": "2025-02-23T11:45:21.299362Z",
     "iopub.status.idle": "2025-02-23T11:45:21.305049Z",
     "shell.execute_reply": "2025-02-23T11:45:21.304140Z",
     "shell.execute_reply.started": "2025-02-23T11:45:21.299704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively converts nested objects to dictionaries.\n",
    "    Preserves str, int, and float values while converting everything else to dict.\n",
    "\n",
    "    Args:\n",
    "        obj: Any Python object\n",
    "\n",
    "    Returns:\n",
    "        Converted structure with only dict, list, str, int, float as types\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (str, int, float)):\n",
    "        return obj\n",
    "\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        return [convert_to_dict(item) for item in obj]\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_to_dict(value) for key, value in obj.items()}\n",
    "    if hasattr(obj, \"__dict__\"):\n",
    "        return convert_to_dict(obj.__dict__)\n",
    "\n",
    "    try:\n",
    "        return {\n",
    "            key: convert_to_dict(value)\n",
    "            for key, value in obj.__getattribute__.__self__.__dict__.items()\n",
    "            if not key.startswith(\"_\")\n",
    "        }\n",
    "    except:\n",
    "        return str(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:51:58.385276Z",
     "iopub.status.busy": "2025-02-23T11:51:58.384990Z",
     "iopub.status.idle": "2025-02-23T11:51:58.388961Z",
     "shell.execute_reply": "2025-02-23T11:51:58.388165Z",
     "shell.execute_reply.started": "2025-02-23T11:51:58.385254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = convert_to_dict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:52:32.880737Z",
     "iopub.status.busy": "2025-02-23T11:52:32.880412Z",
     "iopub.status.idle": "2025-02-23T11:52:32.885217Z",
     "shell.execute_reply": "2025-02-23T11:52:32.884233Z",
     "shell.execute_reply.started": "2025-02-23T11:52:32.880711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pretty_print_and_save(dictionary):\n",
    "    \"\"\"\n",
    "    Pretty prints a dictionary and saves it as a JSON file in the specified folder.\n",
    "\n",
    "    :param dictionary: The dictionary to print and save\n",
    "    :param folder: The folder where the JSON file should be saved\n",
    "    :param filename: The name of the JSON file (without extension)\n",
    "    \"\"\"\n",
    "    print(json.dumps(dictionary, indent=4, sort_keys=True))\n",
    "\n",
    "    with open(\"/kaggle/working/output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dictionary, f, indent=4, sort_keys=True)\n",
    "\n",
    "    print(f\"Dictionary saved to /kaggle/working/output.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:52:34.167462Z",
     "iopub.status.busy": "2025-02-23T11:52:34.167190Z",
     "iopub.status.idle": "2025-02-23T11:52:34.172427Z",
     "shell.execute_reply": "2025-02-23T11:52:34.171707Z",
     "shell.execute_reply.started": "2025-02-23T11:52:34.167439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"result\": {\n",
      "        \"answer\": \"The given review expresses concerns about the significance and practicality of the model being studied, despite acknowledging that the paper is clearly written and addresses a conjecture proposed by Tian (2017). The review has a confidence score of 2, indicating that the reviewer is willing to defend their evaluation but may not have fully understood central parts of the paper. The rating score of 6 suggests that the paper is marginally above the acceptance threshold.\\n\\nTo provide counterfactual reasoning, we can analyze similar reviews with varying confidence and rating scores:\\n\\n1. **ICLR_2018_133**: This review has a confidence score of 4 and a rating of 6. The reviewer acknowledges the novelty of the proposed method and its potential for improved performance in certain tasks, but also highlights areas where the method does not perform as expected. The review suggests that addressing these issues could improve the paper's acceptance prospects.\\n\\n2. **ICLR_2017_129**: With a confidence score of 4 and a rating of 6, this review appreciates the technical soundness of the paper but notes limited novelty. The reviewer suggests that the paper would be more valuable if it included released code or novel insights that improve training scalability.\\n\\n3. **ICLR_2019_1001**: This review has a confidence score of 4 and a rating of 5. The reviewer finds the proposed approach simple but raises concerns about the claims made and the lack of quantitative experimental results. The review suggests that addressing these issues and providing more robust experiments could strengthen the paper.\\n\\nCounterfactual reasoning for the given review could involve considering how the paper might be perceived if it addressed the concerns about the model's significance and practicality. For instance, if the paper demonstrated the model's applicability to more complex or practical settings, or if it provided additional insights or experiments that highlight the model's relevance, the reviewer's confidence and rating might increase. Additionally, if the reviewer had a better understanding of the paper's central parts, their confidence score might improve, leading to a more favorable evaluation.\",\n",
      "        \"retriever_result\": {\n",
      "            \"items\": [\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2017_12', 'hedge': 0.04081632653061224, 'certainty': 4.488500595092773, 'conviction': 0.45622146129608154, 'confidence': 3, 'rating': 10, 'review': 'The paper is an empirical study to justify that 1 SGD with smaller batch sizes converges to flatter minima 2 flatter minima have better generalization ability Pros and ConsAlthough there is little novelty in the paper I think the work is of great value in shedding light into some interesting questions around generalization of deep networks SignificanceI think such results may have impact on both theory and practice respectively by suggesting what assumptions are legitimate for real scenarios for building new theories or be used heuristically to develop new algorithms with generalization by smart manipulation of minibatch sizesCommentsEarlier I had some concern about the correctness of a claim made by the authors which is resolved now They had claimed their proposed sharpness criterion is scale invariance They took care of it by removing this claim in the revised version'}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 1.0\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2018_133', 'hedge': 0.0, 'certainty': 4.51829719543457, 'conviction': 0.09816280007362366, 'confidence': 4, 'rating': 6, 'review': '1 SummaryThis paper proposes a recurrent neural network RNN training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably The authors propose to train a forward and backward RNN in parallel The forward RNN predicts forward in time and the backward RNN predicts backwards in time While the forward RNN is trained to predict the next timestep its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step In experiments it is shown that the proposed method improves training speed in terms of number of training iterations achieves 08 CIDEr points improvement over baselines using the proposed training and also achieves improved performance for the task of speech recognition2 Pros Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected Improved performance in speech recognition task The idea is clearly explained and well motivated3 ConsImage captioning experimentIn the experimental section there is an image captioning result in which the proposed method is used on top of two baselines This experiment shows improvement over such baselines however the performance is still worse compared against baselines such as Lu et al 2017 and Yao et al 2016 It would be optimal if the authors can use their training method on such baselines and show improved performance or explain why this cannot be doneUnconditioned generation experimentsIn these experiments sequential pixelbypixel MNIST generation is performed in which the proposed method did not help Because of this two conditioned set ups are performed 1 25 of pixels are given before generation and 2 75 of pixels are given before generation The proposed method performs similar to the baseline in the 25 case and better than the baseline in the 75 case For completeness and to come to a stronger conclusion on how much uncertainty really affects the proposed method this experiment needs a case in which 50 of the pixels are given Observing 25 of the pixels gives almost no information about the identity of the digit and it makes sense that its hard to encode the future however 50 of the pixels give a good idea of what the digit identity is If the authors believe that the 50 case is not necessary please feel free to explain whyAdditional commentsThe method is shown to converge faster compared to the baselines however it is possible that the baseline may finish training faster the authors do acknowledge the additional computation needed in the backward RNNIt would be informative for the research community to see the relationship of training time how long it takes in hours versus how fast it learns iterations taken to learnExperiments on RL planning tasks would be interesting to see Maybe on a simplepredictable environment4 ConclusionThe paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past The proposed method presents a possible way of better modeling the future however some the results do not clearly back up the claim yet The given score will improve if the authors are able to address the stated issuesPOST REBUTTAL RESPONSEThe authors have addressed the comments on the MNIST experiments and show better results however as far as I can see they did not address my concern about the comparisons on the image captioning experiment In the image captioning experiment the authors choose two networks Show  Tell and Soft attention that they improve using the proposed method that end up performing similar to the second best baseline Yao et al 2016 based on Table 3 and their response I requested for the authors to use their method on the best performing baselines ie Yao et al 2016 or Liu et al 2017 or explain why this cannot be done maybe my request was not clearly stated Applying the proposed method on the strong baselines would highlight the authors claims more strongly than just applying on the average performing chosen baselines This request was not addressed and instead the authors just improved the average performing baselines in Table 3 to meet the best baselines Given that the authors were able to improve the results in the sequential MNIST and improve the average baselines my rating improves one point However I still have concerns about this method not being shown to improve the best methods presented in Table 3 which would give a more solid result My rating changes to marginally above threshold for acceptance'}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 1.0\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2017_129', 'hedge': 0.042735042735042736, 'certainty': 4.968155860900879, 'conviction': 0.11820875853300095, 'confidence': 4, 'rating': 6, 'review': 'This paper is technically sound It highlights well the strengths and weaknesses of the proposed simplified modelIn terms of impact its novelty is limited in the sense that the authors did seemingly the right thing and obtained the expected outcomes The idea of modeling deep learning computation is not in itself particularly novel As a companion paper to an open source release of the model it would meet my bar of acceptance in the same vein as a paper describing a novel dataset which might not provide groundbreaking insights yet be generally useful to the communityIn the absence of released code even if the authors promise to release it soon I am more ambivalent since thats where all the value lies It would also be a different story if the authors had been able to use this framework to make novel architectural decisions that improved training scalability in some way and incorporated such new insights in the paperUPDATED code is now available Revised review accordingly'}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 0.996453260209192\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2019_1032', 'hedge': 0.054945054945054944, 'certainty': 3.0230276584625244, 'conviction': -0.28229820728302, 'confidence': 4, 'rating': 5, 'review': 'This paper studies ReLU model or equivalently onelayeroneneuron model for the classification problem This paper shows if the data is linearly separable gradient descent may converge to either a global minimum or a suboptimal local minimum or diverges This paper further studies the implicit bias induced by GD and SGD and shows if they converge they can have a maximum margin solution Comments1 Using ReLU model for linearly separable data doesnt make sense to me When ReLU is used I expect some more complicated separable condition 2 This paper only studies onelayeroneneuron model which is a very restricted setting Its hard to see how this result can be generalized to the multipleneuron case3 The analysis follows closely with previous work in studying the implicit bias for linear models'}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 0.9959934976437168\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2019_1001', 'hedge': 0.008620689655172414, 'certainty': 4.723076343536377, 'conviction': 0.013329338282346725, 'confidence': 4, 'rating': 5, 'review': 'The paper proposes an approach to provide contrastive visual explanations for deep neural networks  why the network assigned more confidence to some class A as opposed to some other class B As opposed to the applicability of previous approaches to this problem  the approach is designed to directly answer the contrastive explanations question rather adapting other visual saliency techniques for the same Overall while I find the proposed approach simple  the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same Apart from some flaws in the claims made in the paper the paper is easy to follow and understand Assuming the availability of a latent model over the images of the input distribution the proposed approach is directly applicable and faster The authors clearly highlight the problems associated with existing explanation modalities and approaches ranging from ones applicable to only specific deep architectures to ones using backpropagation based heuristics The proposed approach to generate contrastive explanations is simple and is structured along the lines of methods utilizing probe images to explain decisions  except for the added advantage that the provided explanations are instanceagnostic due to the assumption of a latent model over the input distributionComments One of the problems highlighted in the paper regarding existing explanation modalities is the use of another blackbox to explain the decisions of an existing deep network also somewhat of a blackbox which the authors claim their model does not suffer from The proposed approach provides explanations by operating in the latent space of a learned generative model of the input distribution The learned generator in itself is somewhat of a blackbox itself  there has been prior work indicating how much of the input distribution are GANs able to capture As such conditioning on a generative model to propose such contrastive explanations is to some extent using another blackbox generator to explain the decisions of an existing one Thus the above claim made in the paper does not seem wellfounded Furthermore in experiments the paper does not provide any quantitatively convincing results to suggest the generator in use is a good one While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable offtheshelf for any further contrastive explanations regarding any network operating on the same dataset  learning such a model of the input space is an overhead in itself In this light experiments demonstrating comparisons between GANs and VAEs as the reference generative model for explanations would have made the paper stronger as the proposed approach relies explicitly on how good the generative model is  The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain adding square to the topleft of images of class 8 While I like this experiment I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations Typographical Errors Section 31 repeats the use of D for a discriminator as well as the input distribution Procedure 1 and Procedure 2 share the same titles  which is slightly misleading In addition Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same In Section 41 the use of Gradcam and Lime to generate counterfactual explanations is not very clear and makes it slightly hard to follow Citations used for Gradcam are wrong  Sundarajan et al 2016 should be changed to Selvaraju et al 2017Experimental Issues Experimental results are provided only on MNIST and FashionMNIST Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B  experiments on datasets which do not have realimages seem insufficient Additional experiments on at least ImageNet would have made the paper strongerRegarding contrastive explanations experiments on datasets where distractor classes yprobe are present in addition to the class interest ytrue seem important  PASCAL VOC COCO etc Specifically since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting  what needs to change in a region of an image classified as a cat to be classified as a dog while there is an instance of the class  dog present in the image itself Also section 7 in Gradcam httpsarxivorgpdf161002391pdf provides a procedure to generate counterfactual explanations using Gradcam Is there a particular reason the authors did not choose to adopt the above technique as a baseline Experimental results provided in the paper are only qualitative  as such I do not find the comparisons and improvements over the existing approaches convincing enough Since there is no clear metric to evaluate contrastive explanations  human studies to judge the classdiscriminativeness or trust of the proposed approach would have made the paper strongerThe authors adressed the issues raisedcomments made in the review In light of my comments below to the author responses  I am not inclined towards increasing my rating and will stick to my original rating for the paper'}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 0.9936782647247172\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"metadata\": {\n",
      "                \"__retriever\": \"HybridRetriever\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Dictionary saved to /kaggle/working/output.json\n"
     ]
    }
   ],
   "source": [
    "pretty_print_and_save(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T11:55:18.041940Z",
     "iopub.status.busy": "2025-02-23T11:55:18.041582Z",
     "iopub.status.idle": "2025-02-23T11:55:18.049260Z",
     "shell.execute_reply": "2025-02-23T11:55:18.048272Z",
     "shell.execute_reply.started": "2025-02-23T11:55:18.041915Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages:\n",
      "Hedge: 0.0294\n",
      "Certainty: 4.3442\n",
      "Conviction: 0.0807\n",
      "Confidence: 3.8000\n",
      "Rating: 6.4000\n"
     ]
    }
   ],
   "source": [
    "items = result[\"result\"][\"retriever_result\"][\"items\"]\n",
    "\n",
    "total_hedge = total_certainty = total_conviction = total_confidence = total_rating = 0\n",
    "count = len(items)\n",
    "\n",
    "for item in items:\n",
    "    content = eval(item[\"content\"])\n",
    "    total_hedge += content[\"hedge\"]\n",
    "    total_certainty += content[\"certainty\"]\n",
    "    total_conviction += content[\"conviction\"]\n",
    "    total_confidence += content[\"confidence\"]\n",
    "    total_rating += content[\"rating\"]\n",
    "\n",
    "average_hedge = total_hedge / count\n",
    "average_certainty = total_certainty / count\n",
    "average_conviction = total_conviction / count\n",
    "average_confidence = total_confidence / count\n",
    "average_rating = total_rating / count\n",
    "\n",
    "print(\"Averages:\")\n",
    "print(f\"Hedge: {average_hedge:.4f}\")\n",
    "print(f\"Certainty: {average_certainty:.4f}\")\n",
    "print(f\"Conviction: {average_conviction:.4f}\")\n",
    "print(f\"Confidence: {average_confidence:.4f}\")\n",
    "print(f\"Rating: {average_rating:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T12:13:05.518644Z",
     "iopub.status.busy": "2025-02-23T12:13:05.518275Z",
     "iopub.status.idle": "2025-02-23T12:13:05.975610Z",
     "shell.execute_reply": "2025-02-23T12:13:05.974648Z",
     "shell.execute_reply.started": "2025-02-23T12:13:05.518619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828b0bb7876042b58bd0f46863d58517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d3c185680c4b6fbd7aaf23dfc7bed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.878462621413623"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_consistency(\n",
    "    get_conviction_score(rev1.review),\n",
    "    get_certainity(rev1.review),\n",
    "    average_rating,\n",
    "    average_confidence,\n",
    "    get_hedge(rev1.review),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:02:59.893455Z",
     "iopub.status.busy": "2025-02-23T13:02:59.893108Z",
     "iopub.status.idle": "2025-02-23T13:02:59.897567Z",
     "shell.execute_reply": "2025-02-23T13:02:59.896560Z",
     "shell.execute_reply.started": "2025-02-23T13:02:59.893425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "review = \"This paper targets at compressing RNN models to reduce model size, computation cost and enhance model efficiency, without  too much performance drop. This paper proposes to utilize block circulant matrix to speed up the computation as BC matrix multiplication can be computed fast in FFT domain. The idea is straightforward. It seems similar idea has been explored for compressing CNN models  (Ding et al., 2017). But this should be the first work applying BC matrix to compress RNN models. The performance is much better than ESE, in terms of both model computation efficiency and performance in speech applications. The paper is written well. The only issue is it is not clear how BC matrix can be used to compress LSTM, considering LSTM has several different gates with complex interaction. I understand the space is limited but would appreciate if the authors would provide more implementation details for LSTM.\"\n",
    "rating = \"7: Good paper, accept\"\n",
    "rating_score = 7\n",
    "confidence = \"4: The reviewer is confident but not absolutely certain that the evaluation is correct\"\n",
    "confidence_score = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:03:02.707492Z",
     "iopub.status.busy": "2025-02-23T13:03:02.707194Z",
     "iopub.status.idle": "2025-02-23T13:03:02.711534Z",
     "shell.execute_reply": "2025-02-23T13:03:02.710772Z",
     "shell.execute_reply.started": "2025-02-23T13:03:02.707442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rev2 = Review(\n",
    "    review=review,\n",
    "    rating=rating,\n",
    "    rating_score=rating_score,\n",
    "    confidence=confidence,\n",
    "    confidence_score=confidence_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:04:14.668696Z",
     "iopub.status.busy": "2025-02-23T13:04:14.668358Z",
     "iopub.status.idle": "2025-02-23T13:04:15.167843Z",
     "shell.execute_reply": "2025-02-23T13:04:15.166981Z",
     "shell.execute_reply.started": "2025-02-23T13:04:14.668660Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4ac6ccb43c4ac588234ce7d17bf129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f84e801754c5d86a8aeef0d0c0f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.010416666666666666, 0.6182575225830078, 4.3831954)\n"
     ]
    }
   ],
   "source": [
    "print(get_everything(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:03:27.651358Z",
     "iopub.status.busy": "2025-02-23T13:03:27.651041Z",
     "iopub.status.idle": "2025-02-23T13:03:28.131796Z",
     "shell.execute_reply": "2025-02-23T13:03:28.130833Z",
     "shell.execute_reply.started": "2025-02-23T13:03:27.651329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778bda7e61b14907a95dcd6ec5976f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a42c49f9e7248e8b514d6661e627e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.8467443887041615"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_consistency(\n",
    "    get_conviction_score(rev2.review),\n",
    "    get_certainity(rev2.review),\n",
    "    rev2.rating_score,\n",
    "    rev2.confidence_score,\n",
    "    get_hedge(rev2.review),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:05:39.295192Z",
     "iopub.status.busy": "2025-02-23T13:05:39.294871Z",
     "iopub.status.idle": "2025-02-23T13:05:53.124920Z",
     "shell.execute_reply": "2025-02-23T13:05:53.124284Z",
     "shell.execute_reply.started": "2025-02-23T13:05:39.295167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "res = nlp(rev2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:06:26.012626Z",
     "iopub.status.busy": "2025-02-23T13:06:26.012291Z",
     "iopub.status.idle": "2025-02-23T13:06:26.018232Z",
     "shell.execute_reply": "2025-02-23T13:06:26.017434Z",
     "shell.execute_reply.started": "2025-02-23T13:06:26.012597Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"result\": {\n",
      "        \"answer\": \"The review in question discusses a paper that focuses on compressing RNN models using block circulant matrices to enhance model efficiency while maintaining performance. The reviewer acknowledges the novelty of applying this technique to RNNs, despite its previous use in CNNs, and appreciates the paper's clarity. However, they express a desire for more details on how the method applies to LSTMs, given their complexity.\\n\\nCounterfactual reasoning involves considering how changes in certain aspects of the paper or review could affect the confidence and rating scores. Here are some potential counterfactual scenarios:\\n\\n1. **Increased Detail on LSTM Implementation**: If the paper had provided more comprehensive details on implementing block circulant matrices in LSTMs, the reviewer's confidence might have increased. This additional information could have addressed the reviewer's concerns about the complexity of LSTM gates, potentially leading to a higher confidence score, possibly closer to 5, indicating greater certainty in the evaluation.\\n\\n2. **Comparison with More Baselines**: If the paper had included a broader comparison with other RNN compression techniques beyond ESE, the reviewer might have felt more assured about the paper's contributions. This could have resulted in a higher confidence score and possibly a higher rating, as the paper would demonstrate a more thorough evaluation of its effectiveness.\\n\\n3. **Addressing Similarity to Previous Work**: The reviewer notes the similarity to previous work on CNNs. If the paper had explicitly addressed these similarities and differentiated its approach more clearly, it might have strengthened the perception of originality. This could have led to a higher rating, as the paper would be seen as making a more distinct contribution to the field.\\n\\n4. **Empirical Results on Diverse Tasks**: If the paper had included empirical results on a wider range of tasks beyond speech applications, demonstrating the generalizability of the approach, the reviewer might have rated the paper higher. This would show the method's applicability across different domains, potentially increasing both the confidence and rating scores.\\n\\nOverall, the review is positive, with a good rating of 7, indicating acceptance. However, addressing the reviewer's concerns and providing additional details could have further strengthened the paper's evaluation.\",\n",
      "        \"retriever_result\": {\n",
      "            \"items\": [\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2017_103', 'hedge': 0.010638297872340425, 'certainty': 4.932777404785156, 'conviction': 0.025709962472319603, 'confidence': 4, 'rating': 7, 'review': 'This paper introduces a novel RNN architecture named QRNNQNNs are similar to gated RNN  however their gate and state update  functions depend only on the recent input values it does not depend on the previous hidden state The gate and state update functions are computed through a temporal convolution applied on the inputConsequently QRNN allows for more parallel computation since they have less  operations in their hiddentohidden transition depending on the previous hidden state compared to a GRU or LSTM However they possibly loose in expressiveness relatively to those models For instance it is not clear how such a model deals with longterm dependencies without having to stack up several QRNN layersVarious extensions of QRNN leveraging Zoneout Denselyconnected or seq2seq with attention are also proposedAuthors evaluate their approach on various tasks and datasets sentiment classification worldlevel language modelling and character level machine translation Overall the paper is an enjoyable read and the proposed approach is interestingPros Address an important problem Nice empirical evaluation showing the benefit of their approach Demonstrate up to 16x speedup relatively to a LSTMCons Somewhat incremental novelty compared to Balduzizi et al 2016Few specific questions Is densely layer necessary to obtain good result on the IMDB task How does a simple 2layer QRNN compare with 2layer LSTM   How does the ifoifo pooling perform comparatively  How does QRNN deal with longterm time depency Did you try on it on simple toy task such as the copy or the adding task '}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 1.0\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2019_1021', 'hedge': 0.0, 'certainty': 4.83319091796875, 'conviction': 0.5425293445587158, 'confidence': 4, 'rating': 5, 'review': ' Summary The paper proposes to learn transition models for MDPs in terms of objects and their interactions These models are effectively deterministic and are compatible with algorithms for planning with countbased exploration The paper demonstrates the performance of one such planning method in toy tasks and in Pitfall as well as a comparison with other planning methods in the toy tasks The proposed modelbased method called SOORL yields agents that perform better on Pitfall with a small amount of data Assessment As a positive the results of the paper are favorable compared to previous work with good sample efficiency and they demonstrate the viability of the proposed approach The most negative point is that SOORL relies on limiting domainspecific biases that are hard to remove or circumvent Clarity The paper is somewhat clear There are many typos and mistakes in writing and at parts for example the second paragraph of Section 42 the explanations are not clear Originality I believe the work is original The paper explores a natural idea and the claimsresults are not surprising but as far as I am aware it has not been tried before Support The paper provides support for some of the claims made The comparison to related work contains unsupported claims we studied how imperfect planning can affect exploration and could be more upfront about the weaknesses of the proposed method The claims in the introduction are sufficiently supported Significance It would be hard to scale SOORL to other tasks so it is unlikely to be adopted where endtoend learning is wanted Therefore I believe the impact of the paper to be limitedThere is also the question of whether the paper will attract interest and people will work on addressing the limitations of SOORL I would like to hear more from the authors on this point For the rebuttal My greatest doubt is whether the paper will attract enough interest if published and it would be helpful to hear from the authors on why they think future work will build on the paper Why is the proposed approach a step in the right direction Comments Sample efficiency The paper should be more clear about this point It seems that 50 episodes were used for getting the positive reward in Pitfall which is greatObject detection I am happy with the motivation about how we can remove the handmade object detection It is important the other strong assumptions object interaction matrix for example can be removed as well My opinion on simplifications is this They are ok if they are being used to make experiments viable and they can be removed when scaling up but they are not ok if there is no clear way to remove themKnown interaction matrix It may be possible to remove this requirement using the tools in 1Deterministic model The use of noops to make the model deterministic seems right if the ultimate goal is to make the model deterministic but it seems unsuited if the model is to be used for control Maybe the model needs to be temporally extended as I thought the paper was proposing in Section 42 but Section 43 suggests that this temporal extension was not a good idea Is my understanding correctExploration I was a bit confused about how the text discusses exploration UCT uses OFU but the text suggests that it does not What are the components for exploration Both a bonus on unseen transitions and the confidence interval bonus Also the paper would have to provide support for the claim that with limited number of rollouts the agent might not observe the optimistic part of the model in contrast to optimistic MCTS where optimism is build into every node of the tree However it is fair to say that in the to domains MCTS seemed has performed better and for that reason it has been chosen instead of Thompson Sampling for the later experimentsWriting The paper has a number of typos and mistakes that need to be fixed To point out a few I would suggest more careful use of much and very For citations Diuk et al 2008 also proposed and UCT Kocsis  Szepesvari 2006Claims I think the claims made in the introduction could be stated more clearly in the conclusion Intro We show how to do approximate planning  Conclusion Our model learning produces effectively deterministic models that can then be used by usual planning algorithms References 1 Santoro et al 2017 A simple neural network module for relational reasoning'}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 1.0\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2017_103', 'hedge': 0.0, 'certainty': 4.957724571228027, 'conviction': 0.13528850674629211, 'confidence': 4, 'rating': 6, 'review': 'This paper introduces the QuasiRecurrent Neural Network QRNN that dramatically limits the computational burden of the temporal transitions insequence data Briefly and slightly inaccurately model starts with the LSTM structure but removes all but the diagonal elements to the transitionmatrices It also generalizes the connections from lower layers to upper layers to general convolutions in time the standard LSTM can be though of as a convolution with a receptive field of 1 timestep As discussed by the authors the model is related to a number of other recent modifications of RNNs in particular ByteNet and stronglytyped RNNs TRNN In light of these existing models the novelty of the QRNN is somewhat diminished however in my opinion their is still sufficient novelty to justify publicationThe authors present a reasonably solid set of empirical results that support the claims of the paper It does indeed seem that this particular modification of the LSTM warrants attention from others While I feel that the contribution is somewhat incremental I recommend acceptance '}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 0.9983114539705663\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2018_107', 'hedge': 0.018518518518518517, 'certainty': 4.903489589691162, 'conviction': 0.19510847330093384, 'confidence': 4, 'rating': 7, 'review': 'The authors proposed to compress word embeddings by approximate matrix factorization and to solve the problem with the Gumbelsoft trick The proposed method achieved compression rate 98 in a sentiment analysis task and compression rate over 94 in machine translation tasks without a performance loss This paper is wellwritten and easy to follow  The motivation is clear and the idea is simple and effectiveIt would be better to provide deeper analysis in Subsection 61 The current analysis is too simple It may be interesting to explain the meanings of individual components Does each component is related to a certain topic Is it meaningful to perform ADD or SUBSTRACT on the leaned code It may also be interesting to provide suitable theoretical analysis eg relationships with the SVD of the embedding matrix'}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 0.997765159666926\n",
      "                    }\n",
      "                },\n",
      "                {\n",
      "                    \"content\": \"{'id': 'ICLR_2019_1003', 'hedge': 0.0, 'certainty': 4.018031120300293, 'conviction': -0.11436791718006134, 'confidence': 4, 'rating': 3, 'review': 'In this paper the authors propose a dynamic convolution model by exploiting the interscene similarity The computation cost is reduced significantly by reusing the feature map In general the paper is present clearly but the technical contribution is rather incremental I have several concerns1 The authors should further clarify their advantages over the popular framework of CNNLSTM Actually I did not see it 2  What is the difference between the proposed method and applying incremental learning on CNN3 The proposed method reduced the computation in which phase training or tesing4 The experimental section is rather weak The authors should make more comprehensive evaluation on the larger dataset Currently the authors only use some small dataset with short videos which makes the acceleration unnecessary '}\",\n",
      "                    \"metadata\": {\n",
      "                        \"score\": 0.9966394623139703\n",
      "                    }\n",
      "                }\n",
      "            ],\n",
      "            \"metadata\": {\n",
      "                \"__retriever\": \"HybridRetriever\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "Dictionary saved to /kaggle/working/output.json\n"
     ]
    }
   ],
   "source": [
    "result = convert_to_dict(res)\n",
    "pretty_print_and_save(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:06:30.065340Z",
     "iopub.status.busy": "2025-02-23T13:06:30.065053Z",
     "iopub.status.idle": "2025-02-23T13:06:30.072927Z",
     "shell.execute_reply": "2025-02-23T13:06:30.072199Z",
     "shell.execute_reply.started": "2025-02-23T13:06:30.065317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages:\n",
      "Hedge: 0.0058\n",
      "Certainty: 4.7290\n",
      "Conviction: 0.1569\n",
      "Confidence: 4.0000\n",
      "Rating: 5.6000\n"
     ]
    }
   ],
   "source": [
    "items = result[\"result\"][\"retriever_result\"][\"items\"]\n",
    "\n",
    "total_hedge = total_certainty = total_conviction = total_confidence = total_rating = 0\n",
    "count = len(items)\n",
    "\n",
    "for item in items:\n",
    "    content = eval(item[\"content\"])\n",
    "    total_hedge += content[\"hedge\"]\n",
    "    total_certainty += content[\"certainty\"]\n",
    "    total_conviction += content[\"conviction\"]\n",
    "    total_confidence += content[\"confidence\"]\n",
    "    total_rating += content[\"rating\"]\n",
    "\n",
    "average_hedge = total_hedge / count\n",
    "average_certainty = total_certainty / count\n",
    "average_conviction = total_conviction / count\n",
    "average_confidence = total_confidence / count\n",
    "average_rating = total_rating / count\n",
    "\n",
    "print(\"Averages:\")\n",
    "print(f\"Hedge: {average_hedge:.4f}\")\n",
    "print(f\"Certainty: {average_certainty:.4f}\")\n",
    "print(f\"Conviction: {average_conviction:.4f}\")\n",
    "print(f\"Confidence: {average_confidence:.4f}\")\n",
    "print(f\"Rating: {average_rating:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-23T13:09:04.575348Z",
     "iopub.status.busy": "2025-02-23T13:09:04.575034Z",
     "iopub.status.idle": "2025-02-23T13:09:05.066487Z",
     "shell.execute_reply": "2025-02-23T13:09:05.065244Z",
     "shell.execute_reply.started": "2025-02-23T13:09:04.575325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58c66e23e3a46a486b8e600601a280b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa72435d4664cfa98b251469ef96dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6.154005760316711"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_consistency(\n",
    "    get_conviction_score(rev2.review),\n",
    "    get_certainity(rev2.review),\n",
    "    average_rating,\n",
    "    average_confidence,\n",
    "    get_hedge(rev2.review),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
